import json
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple
import requests
import time
from pathlib import Path
import warnings
import os
import pickle
import re
warnings.filterwarnings('ignore')

# ä½¿ç”¨è½»é‡çº§çš„Sentence Transformeræ¨¡å‹
class LocalEmbedder:
    def __init__(self, model_name: str = "./qwen3-embed-0.6b"):
        try:
            from sentence_transformers import SentenceTransformer
            # ä½¿ç”¨è½»é‡çº§æ¨¡å‹ï¼Œå‡å°‘å†…å­˜å ç”¨
            self.model = SentenceTransformer(model_name)
            print(f"æœ¬åœ°åµŒå…¥æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"åŠ è½½æœ¬åœ°æ¨¡å‹å¤±è´¥: {e}")
            self.model = None
    
    def embed(self, texts: List[str]) -> np.ndarray:
        if not self.model:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šéšæœºå‘é‡
            return np.random.randn(len(texts), 384)
        
        # Sentence Transformers è‡ªåŠ¨å¤„ç†æ‰¹å¤„ç†å’Œæ ‡å‡†åŒ–
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        return embeddings

# å‘é‡å­˜å‚¨å’Œæ£€ç´¢ - ä½¿ç”¨npyæ ¼å¼ä¼˜åŒ–å­˜å‚¨
class VectorStore:
    def __init__(self, embedder, index_dir: str = "vector_index"):
        self.embedder = embedder
        self.embeddings = None
        self.questions = []
        self.answers = []
        self.data = []
        self.index_dir = index_dir
        os.makedirs(index_dir, exist_ok=True)
    
    def load_json_data(self, json_path: str):
        """åŠ è½½æ–°çš„JSONæ ¼å¼çš„QAæ•°æ®"""
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            if 'paraname' in item and 'content' in item:
                self.questions.append(item['paraname'])
                self.answers.append(item['content'])
                self.data.append(item)
        
        print(f"åŠ è½½äº† {len(self.questions)} ä¸ªæ®µè½å¯¹")
    
    def save_index(self):
        """ä¿å­˜å‘é‡ç´¢å¼•åˆ°npyæ–‡ä»¶"""
        if self.embeddings is None:
            print("æ²¡æœ‰ç´¢å¼•å¯ä¿å­˜")
            return
        
        # ä¿å­˜åµŒå…¥å‘é‡
        np.save(os.path.join(self.index_dir, 'embeddings.npy'), self.embeddings)
        
        # ä¿å­˜æ–‡æœ¬æ•°æ®
        index_data = {
            'questions': self.questions,
            'answers': self.answers,
            'data': self.data
        }
        
        with open(os.path.join(self.index_dir, 'metadata.pkl'), 'wb') as f:
            pickle.dump(index_data, f)
        
        print(f"å‘é‡ç´¢å¼•å·²ä¿å­˜åˆ° {self.index_dir}")
    
    def load_index(self) -> bool:
        """ä»npyæ–‡ä»¶åŠ è½½å‘é‡ç´¢å¼•"""
        embeddings_path = os.path.join(self.index_dir, 'embeddings.npy')
        metadata_path = os.path.join(self.index_dir, 'metadata.pkl')
        
        if not os.path.exists(embeddings_path) or not os.path.exists(metadata_path):
            print("ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨")
            return False
        
        try:
            # åŠ è½½åµŒå…¥å‘é‡
            self.embeddings = np.load(embeddings_path)
            
            # åŠ è½½å…ƒæ•°æ®
            with open(metadata_path, 'rb') as f:
                metadata = pickle.load(f)
            
            self.questions = metadata['questions']
            self.answers = metadata['answers']
            self.data = metadata['data']
            
            print(f"ä» {self.index_dir} åŠ è½½äº†å‘é‡ç´¢å¼•ï¼ŒåŒ…å« {len(self.questions)} ä¸ªæ®µè½å¯¹")
            return True
        except Exception as e:
            print(f"åŠ è½½ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def build_index(self, force_rebuild: bool = False, batch_size: int = 32):
        """æ„å»ºå‘é‡ç´¢å¼•ï¼Œæ”¯æŒæ‰¹é‡å¤„ç†å‡å°‘å†…å­˜å‹åŠ›"""
        if not force_rebuild and self.load_index():
            return
        
        if not self.questions:
            print("æ²¡æœ‰æ•°æ®å¯ç´¢å¼•")
            return
        
        print("å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")
        
        # åˆ†æ‰¹å¤„ç†ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        all_embeddings = []
        for i in range(0, len(self.questions), batch_size):
            batch_texts = self.questions[i:i + batch_size]
            batch_embeddings = self.embedder.embed(batch_texts)
            all_embeddings.append(batch_embeddings)
            print(f"å·²å¤„ç† {min(i + batch_size, len(self.questions))}/{len(self.questions)} æ¡æ•°æ®")
        
        self.embeddings = np.vstack(all_embeddings)
        print(f"å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {self.embeddings.shape}")
        
        # ä¿å­˜æ–°æ„å»ºçš„ç´¢å¼•
        self.save_index()
    
    def search_with_threshold(self, query: str, similarity_threshold: float = 0.3, max_results: int = 100) -> List[Dict]:
        """åŸºäºç›¸ä¼¼åº¦é˜ˆå€¼æœç´¢ï¼Œè€Œä¸æ˜¯å›ºå®štop_k"""
        if self.embeddings is None:
            self.build_index()
        
        query_embedding = self.embedder.embed([query])[0]
        
        # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        query_norm = np.linalg.norm(query_embedding)
        embeddings_norm = np.linalg.norm(self.embeddings, axis=1)
        
        # é¿å…é™¤é›¶é”™è¯¯
        similarities = np.dot(self.embeddings, query_embedding) / (
            embeddings_norm * query_norm + 1e-8
        )
        
        # è·å–æ‰€æœ‰è¶…è¿‡é˜ˆå€¼çš„ç´¢å¼•
        above_threshold_indices = np.where(similarities >= similarity_threshold)[0]
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é™åˆ¶æœ€å¤§æ•°é‡
        sorted_indices = above_threshold_indices[np.argsort(similarities[above_threshold_indices])[::-1]]
        selected_indices = sorted_indices[:max_results]
        
        results = []
        for idx in selected_indices:
            results.append({
                'question': self.questions[idx],
                'answer': self.answers[idx],
                'similarity': similarities[idx],
                'original_data': self.data[idx]
            })
        
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼åº¦ >= {similarity_threshold} çš„ç»“æœ")
        return results

# DeepSeek APIå®¢æˆ·ç«¯ï¼ˆæ·»åŠ é‡è¯•å’Œè¶…æ—¶å¤„ç†ï¼‰
class DeepSeekClient:
    def __init__(self, api_key: str, base_url: str = "https://api.deepseek.com/v1"):
        self.api_key = api_key
        self.base_url = base_url.strip()  # ä¿®å¤base_urlæœ«å°¾ç©ºæ ¼é—®é¢˜
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        print(f"DeepSeekå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆï¼ŒAPIå¯†é’¥: {api_key[:8]}...")  # è°ƒè¯•ä¿¡æ¯
    
    def chat_completion(self, messages: List[Dict], model: str = "deepseek-chat", 
                       temperature: float = 0.7, max_tokens: int = 2000, max_retries: int = 3) -> str:
        """è°ƒç”¨DeepSeek APIï¼Œæ·»åŠ é‡è¯•æœºåˆ¶"""
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": False
        }
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºè¯·æ±‚æ‘˜è¦
        user_message = next((msg['content'][:50] + '...' for msg in messages if msg['role'] == 'user'), '')
        print(f"æ­£åœ¨è°ƒç”¨DeepSeek APIï¼Œç”¨æˆ·æ¶ˆæ¯: {user_message}")
        
        for attempt in range(max_retries):
            try:
                print(f"APIè¯·æ±‚å°è¯• {attempt + 1}/{max_retries}...")
                start_time = time.time()
                
                response = requests.post(
                    f"{self.base_url}/chat/completions",
                    headers=self.headers,
                    json=payload,
                    timeout=90  # å¢åŠ è¶…æ—¶æ—¶é—´
                )
                
                response_time = time.time() - start_time
                print(f"APIå“åº”æ—¶é—´: {response_time:.2f}ç§’ï¼ŒçŠ¶æ€ç : {response.status_code}")
                
                response.raise_for_status()
                result = response.json()["choices"][0]["message"]["content"]
                print("APIè°ƒç”¨æˆåŠŸï¼")
                return result
                
            except requests.exceptions.Timeout:
                print(f"âš ï¸ APIè¯·æ±‚è¶…æ—¶ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                    print(f"ç­‰å¾… {wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    return "é”™è¯¯: APIè¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥"
                    
            except requests.exceptions.ConnectionError:
                print(f"âš ï¸ ç½‘ç»œè¿æ¥é”™è¯¯ (å°è¯• {attempt + 1}/{max_retries})")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    return "é”™è¯¯: ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè®¾ç½®"
                    
            except Exception as e:
                print(f"âŒ APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(1)
                else:
                    return f"é”™è¯¯: {str(e)}"
        
        return "é”™è¯¯: APIè°ƒç”¨å¤±è´¥ï¼Œè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°"

# å¤šæ–‡ä»¶å‘é‡å­˜å‚¨ç®¡ç†å™¨
class MultiVectorStoreManager:
    def __init__(self, json_files: List[str], embedder, base_index_dir: str = "vector_indexes"):
        self.json_files = json_files
        self.embedder = embedder
        self.base_index_dir = base_index_dir
        self.stores = {}
        os.makedirs(base_index_dir, exist_ok=True)
        
        # ä¸ºæ¯ä¸ªJSONæ–‡ä»¶åˆ›å»ºå¯¹åº”çš„å‘é‡å­˜å‚¨
        for json_file in json_files:
            name = os.path.splitext(os.path.basename(json_file))[0]
            index_dir = os.path.join(base_index_dir, name)
            self.stores[name] = VectorStore(embedder, index_dir)
    
    def load_and_build_all_indexes(self, force_rebuild: bool = False):
        """åŠ è½½å¹¶æ„å»ºæ‰€æœ‰å‘é‡ç´¢å¼•"""
        for name, store in self.stores.items():
            json_file = f"{name}.json"
            if os.path.exists(json_file):
                print(f"æ­£åœ¨å¤„ç† {json_file}...")
                store.load_json_data(json_file)
                store.build_index(force_rebuild=force_rebuild, batch_size=16)
            else:
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°æ–‡ä»¶ {json_file}")
    
    def search_all_stores(self, query: str, similarity_threshold: float = 0.3, max_results_per_store: int = 100) -> List[Dict]:
        """åœ¨æ‰€æœ‰å­˜å‚¨ä¸­æœç´¢"""
        all_results = []
        for name, store in self.stores.items():
            print(f"åœ¨ {name} ä¸­æœç´¢...")
            results = store.search_with_threshold(query, similarity_threshold, max_results_per_store)
            # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ æ¥æºä¿¡æ¯
            for result in results:
                result['source'] = name
            all_results.extend(results)
        return all_results

# æ–‡ç« ç”Ÿæˆç³»ç»Ÿï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
class ArticleGenerator:
    def __init__(self, json_files: List[str], deepseek_api_key: str, 
                 similarity_threshold: float = 0.3,  # ç›¸ä¼¼åº¦é˜ˆå€¼
                 max_iterations: int = 100,  # æœ€å¤§è¿­ä»£æ¬¡æ•°å¢åŠ åˆ°100
                 max_context_length: int = 100000,  # ä¸Šä¸‹æ–‡é•¿åº¦ä¸Šé™
                 base_index_dir: str = "vector_indexes"):
        # ä½¿ç”¨æ›´è½»é‡çš„æ¨¡å‹
        self.embedder = LocalEmbedder("./qwen3-embed-0.6b")
        self.vector_manager = MultiVectorStoreManager(json_files, self.embedder, base_index_dir)
        self.deepseek_client = DeepSeekClient(deepseek_api_key)

        self.info_insufficient_flag = True # æ–°å¢ï¼šç”¨äºè®°å½•æ˜¯å¦æ›¾åˆ¤æ–­ä¿¡æ¯ä¸è¶³
        
        # åŠ è½½æ•°æ®
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        self.vector_manager.load_and_build_all_indexes()
        
        # åŠ¨æ€åˆ†ææ–‡æ¡£ç±»å‹
        self.document_info = self._analyze_document_types()
        
        self.similarity_threshold = similarity_threshold
        self.max_iterations = max_iterations
        self.max_context_length = max_context_length
        self.collected_context = []  # å­˜å‚¨å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«contentï¼‰
        self.relevant_paranames = set()  # åªå­˜å‚¨ç›¸å…³çš„paranameï¼Œç”¨äºå»é‡
        self.intermediate_response = ""  # æ–°å¢ï¼šä¸­é—´å›ç­”
        self.all_searched_paranames = []  # æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰æœç´¢åˆ°çš„paranameï¼Œæ— è®ºæ˜¯å¦ç›¸å…³
        self.insufficient_count = 0  # æ–°å¢ï¼šè®°å½•è¿ç»­ä¿¡æ¯ä¸è¶³çš„æ¬¡æ•°
        self.user_input = ""  # æ–°å¢ï¼šå­˜å‚¨ç”¨æˆ·çš„åŸå§‹è¾“å…¥
        print("æ–‡ç« ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _analyze_document_types(self) -> Dict[str, str]:
        """åŠ¨æ€åˆ†ææ–‡æ¡£ç±»å‹ï¼ŒåŸºäºæ–‡æ¡£åå’Œæ–‡æ¡£å†…å®¹"""
        doc_info = {}
        
        for name, store in self.vector_manager.stores.items():
            if not store.data:
                doc_info[name] = "æœªçŸ¥ç±»å‹çš„æŠ€æœ¯æ–‡æ¡£"
                continue
            
            # ä»æ–‡æ¡£å†…å®¹ä¸­æå–æ ·æœ¬è¿›è¡Œåˆ†æ
            sample_data = store.data[:min(10, len(store.data))]  # å–å‰10ä¸ªæ ·æœ¬
            
            # æå–paranameå’Œcontentä¸­çš„å…³é”®è¯
            sample_paranames = [item.get('paraname', '') for item in sample_data]
            sample_contents = [item.get('content', '') for item in sample_data]
            
            # æ„é€ åˆ†ææç¤º
            sample_text = "\n".join([
                f"æ®µè½æ ‡é¢˜: {pn[:100]}...\nå†…å®¹é¢„è§ˆ: {cont[:200]}..."
                for pn, cont in zip(sample_paranames, sample_contents)
            ])
            
            prompt = f"""åŸºäºä»¥ä¸‹æŠ€æœ¯æ–‡æ¡£çš„æ ·æœ¬å†…å®¹å’Œæ–‡ä»¶åï¼Œåˆ†æè¿™æ˜¯ä»€ä¹ˆç±»å‹çš„æŠ€æœ¯æ–‡æ¡£ï¼š

æ–‡ä»¶å: {name}

æ ·æœ¬å†…å®¹:
{sample_text}

è¯·åˆ†æå¹¶å›ç­”ï¼š
1. è¿™æ˜¯ä»€ä¹ˆé¢†åŸŸçš„æŠ€æœ¯æ–‡æ¡£ï¼Ÿï¼ˆå¦‚ï¼šæ— çº¿é€šä¿¡ã€ç½‘ç»œåè®®ã€è½¯ä»¶å¼€å‘ã€äººå·¥æ™ºèƒ½ã€æ•°æ®åº“ç­‰ï¼‰
2. æ¶‰åŠä»€ä¹ˆå…·ä½“æŠ€æœ¯æ ‡å‡†æˆ–è§„èŒƒï¼Ÿï¼ˆå¦‚ï¼šWiFi 6ã€5Gã€TCP/IPã€HTTP/2ã€SQLç­‰ï¼‰
3. æ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå¦‚ï¼šMACå±‚åè®®ã€ç‰©ç†å±‚è§„èŒƒã€å®‰å…¨æœºåˆ¶ã€APIè®¾è®¡ã€ç®—æ³•å®ç°ç­‰ï¼‰
4. è¯¥æ–‡æ¡£çš„ç”¨é€”æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆå¦‚ï¼šæŠ€æœ¯è§„èŒƒã€ç”¨æˆ·æ‰‹å†Œã€å¼€å‘æŒ‡å—ã€å‚è€ƒæ–‡æ¡£ç­‰ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
{{
    "domain": "æŠ€æœ¯é¢†åŸŸ",
    "standard": "æ¶‰åŠçš„æŠ€æœ¯æ ‡å‡†",
    "content_type": "æ–‡æ¡£ä¸»è¦å†…å®¹ç±»å‹",
    "purpose": "æ–‡æ¡£ç”¨é€”",
    "description": "ç®€è¦æè¿°"
}}

åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯æ–‡æ¡£åˆ†æä¸“å®¶ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«æŠ€æœ¯æ–‡æ¡£çš„ç±»å‹å’Œé¢†åŸŸã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            try:
                response = self.deepseek_client.chat_completion(messages, temperature=0.3, max_tokens=400)
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group())
                    doc_info[name] = f"{analysis.get('domain', 'æŠ€æœ¯æ–‡æ¡£')} - {analysis.get('standard', 'æœªçŸ¥æ ‡å‡†')} - {analysis.get('content_type', 'æœªçŸ¥å†…å®¹ç±»å‹')} - {analysis.get('purpose', 'å‚è€ƒæ–‡æ¡£')}"
                    print(f"æ–‡æ¡£ {name} ç±»å‹åˆ†æ: {doc_info[name]}")
                else:
                    doc_info[name] = f"æŠ€æœ¯æ–‡æ¡£ - {name}"
            except Exception as e:
                print(f"åˆ†ææ–‡æ¡£ {name} ç±»å‹å¤±è´¥: {e}")
                doc_info[name] = f"æŠ€æœ¯æ–‡æ¡£ - {name}"
        
        return doc_info
    
    def _get_document_context(self) -> str:
        """è·å–æ–‡æ¡£ç±»å‹ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = "æ£€ç´¢çš„æ–‡æ¡£ç±»å‹åŒ…æ‹¬ï¼š\n"
        for name, doc_desc in self.document_info.items():
            context += f"- {name}: {doc_desc}\n"
        return context

    def clarify_user_intent(self, user_input: str) -> str:
        """æ¾„æ¸…ç”¨æˆ·æ„å›¾ï¼Œç›´åˆ°AIç¡®è®¤æ˜ç¡®"""
        print("å¼€å§‹æ¾„æ¸…ç”¨æˆ·æ„å›¾...")
        
        # åˆå§‹æ¾„æ¸…å¾ªç¯
        clarification_round = 0
        current_input = user_input
        
        while True:
            clarification_round += 1
            
            # æ‰§è¡Œåˆæ­¥æœç´¢
            search_results = self.vector_manager.search_all_stores(
                current_input, 
                similarity_threshold=self.similarity_threshold,
                max_results_per_store=50
            )
            
            # æå–æœç´¢ç»“æœçš„æ ‡é¢˜
            search_titles = [result['question'] for result in search_results[:10]]  # å–å‰10ä¸ª
            titles_text = "\n".join([f"- {title[:100]}..." for title in search_titles]) if search_titles else "æ— ç›¸å…³ç»“æœ"
            
            # æ„é€ æ¾„æ¸…æç¤º
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚è¯·åˆ†æç”¨æˆ·çš„éœ€æ±‚å¹¶åˆ¤æ–­æ˜¯å¦æ˜ç¡®ã€‚

{self._get_document_context()}

å½“å‰ç”¨æˆ·è¾“å…¥: {current_input}

åŸºäºåˆæ­¥æœç´¢ï¼Œç›¸å…³æ ‡é¢˜åŒ…æ‹¬:
{titles_text}

è¯·åˆ¤æ–­ä»¥ä¸‹å‡ ç‚¹ï¼š
1. ç”¨æˆ·çš„æ„å›¾æ˜¯å¦æ˜ç¡®ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰
2. å¦‚æœä¸æ˜ç¡®ï¼Œè¯·æå‡º1-2ä¸ªå…·ä½“é—®é¢˜æ¥å¸®åŠ©æ¾„æ¸…ç”¨æˆ·éœ€æ±‚
3. å¦‚æœæ˜ç¡®ï¼Œè¯·ç®€è¦è¯´æ˜ä½ ç†è§£çš„ç”¨æˆ·éœ€æ±‚

è¯·ä»¥JSONæ ¼å¼å›å¤:
{{
    "intent_clear": true/false,
    "clarification_questions": ["é—®é¢˜1", "é—®é¢˜2"],
    "understood_intent": "ä½ ç†è§£çš„ç”¨æˆ·éœ€æ±‚"
}}

åªè¿”å›JSONæ ¼å¼çš„å“åº”ï¼š"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯æ–‡æ¡£åˆ†æåŠ©æ‰‹ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­ç”¨æˆ·æ„å›¾æ˜¯å¦æ˜ç¡®ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            print(f"æ­£åœ¨è¿›è¡Œç¬¬ {clarification_round} è½®æ„å›¾æ¾„æ¸…...")
            response = self.deepseek_client.chat_completion(messages, temperature=0.3, max_tokens=500)
            
            try:
                import re
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    analysis = json.loads(json_match.group())
                    intent_clear = analysis.get('intent_clear', False)
                    clarification_questions = analysis.get('clarification_questions', [])
                    understood_intent = analysis.get('understood_intent', '')
                    
                    if intent_clear:
                        print(f"âœ… ç”¨æˆ·æ„å›¾å·²æ˜ç¡®: {understood_intent}")
                        return understood_intent
                    elif clarification_questions:
                        print("éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…ç”¨æˆ·æ„å›¾:")
                        for i, question in enumerate(clarification_questions, 1):
                            print(f"  {i}. {question}")
                        
                        # è¯¢é—®ç”¨æˆ·
                        print("\nä¸ºäº†æ›´å¥½åœ°ç†è§£æ‚¨çš„éœ€æ±‚ï¼Œè¯·å›ç­”ä»¥ä¸Šé—®é¢˜:")
                        user_response = input("æ‚¨çš„å›ç­”: ").strip()
                        
                        if user_response:
                            # æ›´æ–°å½“å‰è¾“å…¥ï¼Œç”¨äºä¸‹ä¸€è½®æœç´¢å’Œåˆ†æ
                            current_input = f"{user_input}\n\nç”¨æˆ·è¡¥å……è¯´æ˜: {user_response}"
                        else:
                            print("æœªæ”¶åˆ°æœ‰æ•ˆå›ç­”ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹è¾“å…¥...")
                    else:
                        print("âŒ æ— æ³•è§£ææ„å›¾åˆ†æç»“æœï¼Œç»§ç»­ä½¿ç”¨åŸå§‹è¾“å…¥...")
                        return user_input
                else:
                    print("âŒ æ— æ³•è§£ææ„å›¾åˆ†æç»“æœï¼Œç»§ç»­ä½¿ç”¨åŸå§‹è¾“å…¥...")
                    return user_input
            except Exception as e:
                print(f"âŒ è§£ææ„å›¾åˆ†æç»“æœå¤±è´¥: {e}")
                return user_input
            
            # é™åˆ¶æ¾„æ¸…è½®æ•°ï¼Œé¿å…æ— é™å¾ªç¯
            if clarification_round >= 3:
                print("å·²è¾¾åˆ°æœ€å¤§æ¾„æ¸…è½®æ•°ï¼Œç»§ç»­ä½¿ç”¨å½“å‰ç†è§£çš„æ„å›¾...")
                return current_input

    def generate_search_query(self, user_input: str, context: List[Dict], intermediate_response: str) -> str:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢ï¼Œå¼•å¯¼AIå…³æ³¨å½“å‰ç¼ºå°‘çš„ä¿¡æ¯ç±»å‹"""
        
        # è·å–å·²æœ‰çš„ paranamesï¼ˆç›¸å…³çš„ç»“æœï¼‰
        existing_paranames = [ctx['original_data'].get('paraname', '') for ctx in context if 'original_data' in ctx]
        existing_paranames_preview = "\n".join([f"- {p[:100]}..." for p in existing_paranames[-5:]]) if existing_paranames else "æ— "
        
        # è·å–æ‰€æœ‰æœç´¢åˆ°çš„ paranamesï¼ˆåŒ…æ‹¬ä¸ç›¸å…³çš„ï¼‰
        all_searched_preview = "\n".join([f"- {p[:100]}..." for p in self.all_searched_paranames[-10:]]) if self.all_searched_paranames else "æ— "
    
        # æç¤ºAIåˆ†æå½“å‰ä¿¡æ¯ç¼ºå£å¹¶ç”Ÿæˆé’ˆå¯¹æ€§æŸ¥è¯¢
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„æŠ€æœ¯ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ã€å·²æœ‰ä¿¡æ¯å’Œä¸­é—´å›ç­”ï¼Œç”Ÿæˆä¸€ä¸ª**ç²¾å‡†çš„è¡¥å…¨å¼æœç´¢æŸ¥è¯¢**ã€‚
        
{self._get_document_context()}

ç”¨æˆ·åŸå§‹éœ€æ±‚: {user_input}
        
å·²æœ‰ç›¸å…³ä¿¡æ¯æ ‡é¢˜:
{existing_paranames_preview}
        
å½“å‰ä¸­é—´å›ç­”:
{intermediate_response[:500] + '...' if len(intermediate_response) > 500 else intermediate_response}
        
å†å²ä¸Šæ‰€æœ‰æœç´¢åˆ°çš„æ ‡é¢˜ï¼ˆå¯èƒ½ç›¸å…³ä¹Ÿå¯èƒ½ä¸ç›¸å…³ï¼‰:
{all_searched_preview}
        
å¿…é¡»æ€è€ƒï¼š
1. å½“å‰å·²æœ‰ä¿¡æ¯ä¸»è¦é›†ä¸­åœ¨å“ªäº›æ–¹é¢ï¼Ÿ
2. å·²æœ‰ä¿¡æ¯æ ‡é¢˜æ„å‘³ç€ä»€ä¹ˆå…¨æ™¯ï¼Ÿç”¨æˆ·çš„éœ€æ±‚ä½äºå…¨æ™¯ä¹‹ä¸‹çš„å“ªä¸ªåœ°æ–¹ï¼Ÿ
3. å…¨æ™¯å†…è¿˜å¯èƒ½æœ‰ä»€ä¹ˆï¼Ÿå¯»æ‰¾è§†é‡ä¹‹å¤–çš„å…¨æ™¯ï¼
4. ä¸­é—´å›ç­”æ˜¯å¦å·²ç»è¶³å¤Ÿå…¨é¢ï¼Ÿè¿˜æœ‰å“ªäº›ç¼ºå¤±çš„éƒ¨åˆ†éœ€è¦è¡¥å……ï¼Ÿ
5. å†å²ä¸Šæ‰€æœ‰æœç´¢åˆ°çš„æ ‡é¢˜ä¸­ï¼Œå“ªäº›å¯èƒ½æ˜¯è¢«è¯¯åˆ¤ä¸ºä¸ç›¸å…³çš„ä½†å®é™…æœ‰ç”¨çš„ï¼Ÿ
        
ğŸ’¡ å¿…é¡»è¿™æ ·åšï¼š
- ä½ å®é™…ç”Ÿæˆå‡è®¾çš„æ®µè½æ ‡é¢˜æˆ–è€…å¯¹å‡è®¾çš„æ®µè½å†…å†…å®¹çš„æ€»ç»“æ€§æé—®ï¼Œä¾¿äºå‘é‡æ£€ç´¢åŒ¹é…ã€‚
- ä½ å¯ä»¥ç”Ÿæˆæ•´ä¸ªæ–‡ç« çš„ä»»æ„ç« èŠ‚çš„æ®µè½æ ‡é¢˜æˆ–è€…å¯¹ä»»ä½•æ®µè½å†…å†…å®¹çš„æ€»ç»“æ€§æé—®ï¼Œæ‰€ä»¥å…³é”®è¯éƒ½å¾ˆå¯èƒ½å’Œç”¨æˆ·çš„é—®é¢˜æˆ–è€…å·²æœ‰çš„æ ‡é¢˜ä¸ä¸€æ ·ï¼åªè¦å®ƒä»¬å¯¹ç”¨æˆ·æœ‰åˆ©ï¼
- ä½ å®é™…åœ¨è§’è‰²æ‰®æ¼”ï¼Œå†™æ–‡ç« èµ·æ®µè½æ ‡é¢˜ï¼Œæˆ–è€…èµ„æ·±ä¸“å®¶å¯¹æ®µè½å†…å†…å®¹çš„æ€»ç»“æ€§æé—®ï¼Œå·²æœ‰ä¿¡æ¯æ˜¯çœŸå®çš„ä¾‹å­ä½†æ˜¯å¾ˆå¯èƒ½æ˜¯è¡¨é¢çš„ä¾‹å­ï¼Œä½ éœ€è¦æƒ³è±¡åŠ›ï¼
        
è¾“å‡ºè¦æ±‚ï¼š
- åªè¿”å›ä¸€ä¸ªæ–°çš„æœç´¢æŸ¥è¯¢è¯­å¥ï¼Œä¸éœ€è¦è§£é‡Šã€‚
- æŸ¥è¯¢è¦å…·ä½“ã€ä¸“ä¸šï¼Œèšç„¦äºæŸä¸ªæŠ€æœ¯ç‚¹æˆ–æ–¹å‘ã€‚
"""
    
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯ä¿¡æ¯æ£€ç´¢åŠ©æ‰‹ï¼Œæ“…é•¿åˆ†æä¿¡æ¯ç¼ºå£å¹¶ç”Ÿæˆç²¾å‡†çš„æŠ€æœ¯æœç´¢æŸ¥è¯¢ã€‚"},
            {"role": "user", "content": prompt}
        ]
    
        print("æ­£åœ¨ç”Ÿæˆè¡¥å…¨å¼æœç´¢æŸ¥è¯¢...")
        query = self.deepseek_client.chat_completion(messages, temperature=0.5, max_tokens=120)
        return query.strip('"\'')    

    def filter_relevant_content(self, search_results: List[Dict], user_input: str, intermediate_response: str) -> List[Dict]:
        if not search_results:
            return []
    
        results_text = "\n\n".join([
            f"ç»“æœ {i+1} (ç›¸ä¼¼åº¦: {result['similarity']:.3f}, æ¥æº: {result['source']}):\n"
            f"æ®µè½æ ‡é¢˜: {result['question']}"
            for i, result in enumerate(search_results)
        ])
    
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æœç´¢ç»“æœçš„æ®µè½æ ‡é¢˜æ˜¯å¦ä¸ç”¨æˆ·éœ€æ±‚ç›¸å…³ï¼Œå¹¶ç­›é€‰å‡ºçœŸæ­£ç›¸å…³çš„æ®µè½ã€‚

{self._get_document_context()}

ç”¨æˆ·éœ€æ±‚: {user_input}
    
å½“å‰ä¸­é—´å›ç­”:
{intermediate_response[:500] + '...' if len(intermediate_response) > 500 else intermediate_response}
    
æœç´¢ç»“æœæ®µè½æ ‡é¢˜:
{results_text}
    
âš ï¸ æ³¨æ„äº‹é¡¹ï¼š
- ç›¸ä¼¼åº¦åˆ†æ•°ä»…ä¾›å‚è€ƒï¼Œè¯·å‹¿ä»…ä¾æ®ç›¸ä¼¼åº¦å†³å®šã€‚
- å¿…é¡»ç¡®ä¿æ®µè½ä¸»é¢˜ã€ç”¨æˆ·éœ€æ±‚ã€æ•°æ®åŸå§‹æ¥æºä¸‰è€…ä¸€è‡´æ‰ç®—ä½œç›¸å…³ã€‚
- å¦‚æœæŸä¸ªæ®µè½è™½ç„¶å…³é”®è¯åŒ¹é…ä½†æ¥æºä¸ä¸€è‡´æˆ–è¯­ä¹‰ä¸ç¬¦ï¼Œè¯·æ’é™¤ã€‚
- è€ƒè™‘å½“å‰ä¸­é—´å›ç­”çš„å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦çœŸçš„éœ€è¦è¿™äº›æ–°ä¿¡æ¯ã€‚
- æ£€ç´¢çš„å¯¹è±¡æ˜¯æŠ€æœ¯è§„èŒƒæ–‡æ¡£ä¸­çš„æ®µè½ï¼Œéœ€è¦åŸºäºæŠ€æœ¯äº‹å®åˆ¤æ–­ç›¸å…³æ€§ã€‚
- æ ¹æ®æ–‡æ¡£ç±»å‹ä¿¡æ¯åˆ¤æ–­å†…å®¹çš„ç›¸å…³æ€§ï¼ˆå‚è€ƒä¸Šé¢çš„æ–‡æ¡£ç±»å‹ä¿¡æ¯ï¼‰

ä¾‹å¤–æƒ…å†µï¼š
- å¦‚æœçœ‹è¿‡å»éƒ½ä¸ç›¸å…³ï¼Œè¿™ä¸ªæ—¶å€™ä½ å¯ä»¥å…ˆä¸é¡¾ç”¨æˆ·éœ€æ±‚ï¼ŒæŠŠä½ æ„Ÿå…´è¶£çš„æ®µè½è®¤ä¸ºç›¸å…³ï¼Œä¸‹ä¸€è½®ä¼šç»™ä½ çœ‹æ®µè½å†…çš„å…·ä½“å†…å®¹ã€‚
    
è¯·åˆ†ææ¯ä¸ªæœç´¢ç»“æœçš„ç›¸å…³æ€§ï¼Œå¹¶è¿”å›ä¸€ä¸ªJSONæ ¼å¼çš„å“åº”ï¼ŒåŒ…å«:
1. relevant_indices: ç›¸å…³ç»“æœçš„ç´¢å¼•åˆ—è¡¨ï¼ˆä»1å¼€å§‹ï¼‰
2. reasoning: ç®€è¦è¯´æ˜ç­›é€‰ç†ç”±
    
åªè¿”å›JSONæ ¼å¼çš„å“åº”ï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹ï¼š"""
    
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†…å®¹ç­›é€‰ä¸“å®¶ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­ä¿¡æ¯ç›¸å…³æ€§ã€‚"},
            {"role": "user", "content": prompt}
        ]
    
        print("ğŸ” æ­£åœ¨ä½¿ç”¨AIåˆ¤æ–­å†…å®¹ç›¸å…³æ€§...")
        response = self.deepseek_client.chat_completion(messages, temperature=0.2, max_tokens=800)
    
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                filter_result = json.loads(json_match.group())
                relevant_indices = filter_result.get('relevant_indices', [])
                relevant_results = [search_results[idx - 1] for idx in relevant_indices if 1 <= idx <= len(search_results)]
                print(f"âœ… AIç­›é€‰ç»“æœ: ä» {len(search_results)} ä¸ªç»“æœä¸­ç­›é€‰å‡º {len(relevant_results)} ä¸ªç›¸å…³ç»“æœ")
                return relevant_results
        except Exception as e:
            print(f"âŒ è§£æAIç­›é€‰ç»“æœå¤±è´¥: {e}")
    
        return search_results
    
    def should_stop_search(self, iteration: int, total_context_length: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœæ­¢æœç´¢"""
        if iteration >= self.max_iterations:
            print(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({self.max_iterations})ï¼Œåœæ­¢æœç´¢")
            return True
        
        if total_context_length >= self.max_context_length:
            print(f"ä¸Šä¸‹æ–‡é•¿åº¦è¾¾åˆ°ä¸Šé™ ({total_context_length}å­—ç¬¦)ï¼Œåœæ­¢æœç´¢")
            return True
        
        # å¯ä»¥æ·»åŠ å…¶ä»–åœæ­¢æ¡ä»¶ï¼Œå¦‚è¿ç»­å‡ è½®æ²¡æœ‰æ–°å†…å®¹ç­‰
        return False
    
    def generate_article(self, user_input: str) -> Tuple[str, str, List[str]]:
        """ç”Ÿæˆå®Œæ•´çš„æ–‡ç« ï¼Œè¿”å›æ–‡ç« å†…å®¹ã€æ ‡é¢˜å’Œä½¿ç”¨çš„paranameåˆ—è¡¨"""
        print(f"\nå¼€å§‹å¤„ç†ç”¨æˆ·è¾“å…¥: '{user_input}'")
        
        # é¦–å…ˆæ¾„æ¸…ç”¨æˆ·æ„å›¾
        clarified_intent = self.clarify_user_intent(user_input)
        self.user_input = clarified_intent  # ä¿å­˜æ¾„æ¸…åçš„æ„å›¾

        # é‡ç½®ä¸Šä¸‹æ–‡
        self.collected_context = []
        self.relevant_paranames = set()
        self.intermediate_response = ""
        self.all_searched_paranames = []  # é‡ç½®æ‰€æœ‰æœç´¢åˆ°çš„paranameåˆ—è¡¨
        self.insufficient_count = 0  # é‡ç½®ä¿¡æ¯ä¸è¶³è®¡æ•°å™¨
        iteration = 0
        total_context_length = 0
        
        while not self.should_stop_search(iteration, total_context_length):
            iteration += 1
            print(f"\nå¼€å§‹ç¬¬ {iteration} è½®æœç´¢...")
            
            if iteration == 1:
                # ç¬¬ä¸€è½®ä½¿ç”¨æ¾„æ¸…åçš„æ„å›¾
                query = clarified_intent
            else:
                # åç»­è½®æ¬¡ç”Ÿæˆæ–°çš„æœç´¢æŸ¥è¯¢
                query = self.generate_search_query(clarified_intent, self.collected_context, self.intermediate_response)
                print(f"ç”Ÿæˆçš„æœç´¢æŸ¥è¯¢: '{query}'")
                
                if not query or len(query) < 3:
                    print("æœç´¢æŸ¥è¯¢æ— æ•ˆï¼Œåœæ­¢æœç´¢")
                    break
            
            # æ‰§è¡ŒåŸºäºé˜ˆå€¼çš„æœç´¢
            search_results = self.vector_manager.search_all_stores(
                query, 
                similarity_threshold=self.similarity_threshold,
                max_results_per_store=100
            )
            
            if not search_results:
                print("æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœï¼Œåœæ­¢æœç´¢")
                break
            
            # å°†æ‰€æœ‰æœç´¢åˆ°çš„paranameæ·»åŠ åˆ°å†å²è®°å½•ä¸­
            for result in search_results:
                paraname = result['original_data'].get('paraname', '')
                if paraname and paraname not in self.all_searched_paranames:
                    self.all_searched_paranames.append(paraname)
            
            # ä½¿ç”¨AIç­›é€‰ç›¸å…³çš„å†…å®¹ï¼ˆåªåŸºäºparanameåˆ¤æ–­ï¼‰
            relevant_results = self.filter_relevant_content(search_results, clarified_intent, self.intermediate_response)
            
            if not relevant_results:
                print("AIç­›é€‰åæ²¡æœ‰ç›¸å…³ç»“æœï¼Œåœæ­¢æœç´¢")
                break
            
            # å»é‡å¹¶æ·»åŠ åˆ°æ”¶é›†çš„ä¸Šä¸‹æ–‡ï¼ˆåŸºäºparanameå»é‡ï¼‰
            new_results = []
            
            for result in relevant_results:
                paraname = result['original_data'].get('paraname', '')
                if paraname and paraname not in self.relevant_paranames:
                    self.relevant_paranames.add(paraname)
                    new_results.append(result)
                    total_context_length += len(result['question']) + len(result['answer'])
            
            if not new_results:
                print("æ²¡æœ‰æ–°çš„ç›¸å…³å†…å®¹ï¼Œåœæ­¢æœç´¢")
                break
            
            self.collected_context.extend(new_results)
            print(f"æ–°å¢ {len(new_results)} ä¸ªç›¸å…³ç»“æœï¼Œæ€»ä¸Šä¸‹æ–‡é•¿åº¦: {total_context_length} å­—ç¬¦")
            
            # ç”Ÿæˆä¸­é—´å›ç­”
            self.intermediate_response = self._generate_intermediate_response(clarified_intent, new_results)
            print(f"ç”Ÿæˆä¸­é—´å›ç­”: {self.intermediate_response[:100]}...")
            
            # æ£€æŸ¥æ˜¯å¦ä¿¡æ¯è¶³å¤Ÿï¼ˆç”±AIåˆ¤æ–­ï¼‰
            if self.is_information_sufficient(clarified_intent):
                print("AIåˆ¤æ–­ä¿¡æ¯å·²è¶³å¤Ÿï¼Œåœæ­¢æœç´¢")
                break
        
        # ç”Ÿæˆæœ€ç»ˆæ–‡ç« 
        article, title = self._generate_final_article(clarified_intent)
        return article, title, list(self.relevant_paranames)
    
    def _generate_intermediate_response(self, user_input: str, new_results: List[Dict]) -> str:
        """ç”Ÿæˆä¸­é—´å›ç­”ï¼Œå¸¦ä¸Šä¹‹å‰çš„ä¸­é—´å›ç­”ä½œä¸ºä¸Šä¸‹æ–‡"""
        if not new_results:
            return self.intermediate_response  # æ²¡æœ‰æ–°å†…å®¹åˆ™ä¿æŒä¸Šä¸€è½®çš„å›ç­”
    
        # å‡†å¤‡æ–°è·å–çš„ä¿¡æ¯
        context_str = "\n\n".join([
            f"### ç›¸å…³ä¿¡æ¯ {i+1} (ç›¸ä¼¼åº¦: {ctx['similarity']:.3f}, æ¥æº: {ctx['source']})\n"
            f"**æ®µè½æ ‡é¢˜**: {ctx['original_data']['paraname']}\n"
            f"**å†…å®¹**: {ctx['answer'][:500]}..."
            for i, ctx in enumerate(new_results)
        ])
    
        # æ„é€  promptï¼Œå¸¦ä¸Šä¸Šä¸€è½®çš„ä¸­é—´å›ç­”
        prompt = f"""åŸºäºä»¥ä¸‹æ–°è·å–çš„ä¿¡æ¯å’Œç”¨æˆ·åŸå§‹éœ€æ±‚ï¼Œç»“åˆä½ ä¹‹å‰çš„ä¸­é—´å›ç­”ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ä¸­é—´å›ç­”ä»¥æ›¿æ¢ä½ ä¹‹å‰çš„ä¸­é—´å›ç­”ï¼š

{self._get_document_context()}
    
**ç”¨æˆ·åŸå§‹éœ€æ±‚**: {user_input}
    
**ä½ ä¹‹å‰çš„ä¸­é—´å›ç­”**:
{self.intermediate_response[:1000] + '...' if len(self.intermediate_response) > 1000 else self.intermediate_response}
    
**æ–°è·å–çš„ä¿¡æ¯**:
{context_str}
    
**è¦æ±‚**:
1. å›ç­”è¦å»¶ç»­ä¹‹å‰çš„é€»è¾‘ï¼Œä¸è¦é‡å¤ä¹Ÿä¸è¦çŸ›ç›¾
2. æ•´åˆæ–°ä¿¡æ¯ï¼Œä½¿å†…å®¹æ›´å®Œæ•´
3. ä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§
4. å­—æ•°æ§åˆ¶åœ¨300-500å­—ä¹‹é—´
5. åŸºäºæŠ€æœ¯äº‹å®è¿›è¡Œé™ˆè¿°ï¼Œä¸è¦æ·»åŠ æ¨æµ‹å†…å®¹
6. ç»“åˆæ–‡æ¡£ç±»å‹ç‰¹ç‚¹è¿›è¡Œä¸“ä¸šåˆ†æ
    
è¯·ç›´æ¥è¾“å‡ºä¸­é—´å›ç­”ï¼š"""
    
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æŠ€æœ¯ä¸“å®¶ï¼Œæ“…é•¿æ•´åˆä¿¡æ¯å¹¶ç»™å‡ºä¸“ä¸šçš„ä¸­é—´å›ç­”ã€‚"},
            {"role": "user", "content": prompt}
        ]
    
        print("æ­£åœ¨ç”Ÿæˆä¸­é—´å›ç­”...")
        response = self.deepseek_client.chat_completion(messages, temperature=0.7, max_tokens=1000)
        return response

    def is_information_sufficient(self, user_input: str) -> bool:
        """ç”±AIåˆ¤æ–­æ”¶é›†çš„ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿç”Ÿæˆæ–‡ç« ï¼ŒåªåŸºäºparanameåˆ¤æ–­"""
        if not self.collected_context:
            return False
        
        # å‡†å¤‡ä¸Šä¸‹æ–‡é¢„è§ˆï¼Œåªä½¿ç”¨paraname
        context_preview = "\n".join([
            f"- {ctx['question'][:50]}... (ç›¸ä¼¼åº¦: {ctx['similarity']:.3f}, æ¥æº: {ctx['source']})"
            for ctx in self.collected_context[-5:]  # æ˜¾ç¤ºæœ€å5ä¸ªç»“æœ
        ])
        
        prompt = f"""åŸºäºå½“å‰æ”¶é›†çš„æ®µè½æ ‡é¢˜å’Œä¸­é—´å›ç­”ï¼Œåˆ¤æ–­æ˜¯å¦è¶³å¤Ÿç”Ÿæˆä¸€ç¯‡å…³äº"{user_input}"çš„å®Œæ•´æ–‡ç« ã€‚

{self._get_document_context()}

å·²æ”¶é›†çš„æ®µè½æ ‡é¢˜æ¦‚è¦:
{context_preview}

å½“å‰ä¸­é—´å›ç­”:
{self.intermediate_response[:500] + '...' if len(self.intermediate_response) > 500 else self.intermediate_response}

æ€»å…±æ”¶é›†äº† {len(self.collected_context)} æ¡ç›¸å…³ä¿¡æ¯ã€‚

è¯·åˆ¤æ–­æ˜¯å¦è¿˜éœ€è¦ç»§ç»­æœç´¢æ›´å¤šä¿¡æ¯ï¼Œè¿˜æ˜¯å·²ç»è¶³å¤Ÿç”Ÿæˆé«˜è´¨é‡æ–‡ç« ã€‚è¿”å›JSONæ ¼å¼å“åº”:
{{
    "sufficient": true/false,
    "reason": "ç®€è¦è¯´æ˜ç†ç”±"
}}

åªè¿”å›JSONæ ¼å¼çš„å“åº”ï¼š"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†…å®¹è¯„ä¼°ä¸“å®¶ï¼Œèƒ½å¤Ÿå‡†ç¡®åˆ¤æ–­ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿç”Ÿæˆé«˜è´¨é‡æ–‡ç« ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        print("æ­£åœ¨è¯„ä¼°ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿ...")
        response = self.deepseek_client.chat_completion(messages, temperature=0.2, max_tokens=300)
        
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                assessment = json.loads(json_match.group())
                sufficient = assessment.get('sufficient', False)
                reason = assessment.get('reason', '')
                
                print(f"ä¿¡æ¯å……è¶³æ€§è¯„ä¼°: {'è¶³å¤Ÿ' if sufficient else 'ä¸è¶³'} - {reason}")

                if not sufficient:
                    self.info_insufficient_flag = True  # è®¾ç½®æ ‡å¿—ä½
                    self.insufficient_count += 1  # å¢åŠ è®¡æ•°å™¨
                    
                    # å¦‚æœè¿ç»­ä¸‰æ¬¡ä¿¡æ¯ä¸è¶³ï¼Œè¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
                    if self.insufficient_count >= 3:
                        print("\nâš ï¸ è¿ç»­ä¸‰æ¬¡åˆ¤æ–­ä¿¡æ¯ä¸è¶³ï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤æ˜¯å¦ç«‹å³ç”Ÿæˆæ–‡ç« ")
                        user_choice = input("æ˜¯å¦ç«‹å³ç”Ÿæˆæ–‡ç« ï¼Ÿ(y/n): ").strip().lower()
                        if user_choice in ['y', 'yes', 'æ˜¯']:
                            print("ç”¨æˆ·é€‰æ‹©ç«‹å³ç”Ÿæˆæ–‡ç« ")
                            self.insufficient_count = 0  # é‡ç½®è®¡æ•°å™¨
                            return True  # è®¤ä¸ºä¿¡æ¯è¶³å¤Ÿ
                        else:
                            print("ç”¨æˆ·é€‰æ‹©ä¸ç«‹å³ç”Ÿæˆæ–‡ç« ")
                            return False  # ç¡®å®ä¿¡æ¯ä¸è¶³
                else:
                    self.info_insufficient_flag = False  # è®¾ç½®æ ‡å¿—ä½
                    self.insufficient_count = 0  # é‡ç½®è®¡æ•°å™¨

                return sufficient
        except Exception as e:
            print(f"è§£æä¿¡æ¯å……è¶³æ€§è¯„ä¼°å¤±è´¥: {e}")
        
        # é»˜è®¤ç»§ç»­æœç´¢
        return False
    
    def _extract_title_from_article(self, article: str) -> str:
        """ä»æ–‡ç« å†…å®¹ä¸­æå–æ ‡é¢˜"""
        # æŸ¥æ‰¾Markdownæ ¼å¼çš„æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)$', article, re.MULTILINE)
        if title_match:
            return title_match.group(1).strip()
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°Markdownæ ‡é¢˜ï¼Œå°è¯•æå–ç¬¬ä¸€è¡Œä½œä¸ºæ ‡é¢˜
        first_line = article.split('\n')[0].strip()
        if first_line and len(first_line) < 100:  # ç¡®ä¿ä¸æ˜¯è¿‡é•¿çš„æ–‡æœ¬
            return first_line
        
        # å¦‚æœéƒ½æ— æ³•æå–ï¼Œè¿”å›é»˜è®¤æ ‡é¢˜
        return "ç”Ÿæˆçš„æ–‡ç« "
    
    def _generate_final_article(self, user_input: str) -> Tuple[str, str]:
        """ç”Ÿæˆæœ€ç»ˆçš„æ–‡ç« å†…å®¹ï¼Œå¹¶è¿”å›å†…å®¹å’Œæ ‡é¢˜"""
        print(f"\næ­£åœ¨ç”Ÿæˆæœ€ç»ˆæ–‡ç« ï¼Œä½¿ç”¨ {len(self.collected_context)} ä¸ªä¸Šä¸‹æ–‡ç‰‡æ®µ...")

        if not self.collected_context:
            # æ²¡æœ‰å¯ç”¨çš„ä¸Šä¸‹æ–‡ï¼Œæå‰ç»ˆæ­¢
            message = (
                "# æ— æ³•ç”Ÿæˆæ–‡ç« \n\n"
                "æŠ±æ­‰ï¼Œæ ¹æ®å½“å‰çŸ¥è¯†åº“æœªèƒ½æ£€ç´¢åˆ°ä¸æ‚¨çš„è¾“å…¥â€œ{}â€ç›¸å…³çš„ä»»ä½•å†…å®¹ã€‚\n\n"
                "è¯·å°è¯•æä¾›æ›´å…·ä½“æˆ–ä¸åŒçš„å…³é”®è¯ã€‚"
            ).format(user_input)
            return message, f"æœªæ‰¾åˆ°ç›¸å…³å†…å®¹-{user_input}"       

        # æŒ‰ç›¸ä¼¼åº¦æ’åºå¹¶é€‰æ‹©æœ€ä½³ä¸Šä¸‹æ–‡
        sorted_context = sorted(self.collected_context, key=lambda x: x['similarity'], reverse=True)
        
        # é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ä½†ç¡®ä¿è´¨é‡
        max_context_used = min(10, len(sorted_context))  # æœ€å¤šä½¿ç”¨10ä¸ªæœ€ä½³ä¸Šä¸‹æ–‡
        best_context = sorted_context[:max_context_used]
        
        context_str = "\n\n".join([
            f"### ç›¸å…³ä¿¡æ¯ {i+1} (ç›¸ä¼¼åº¦: {ctx['similarity']:.3f}, æ¥æº: {ctx['source']})\n"
            f"**æ®µè½æ ‡é¢˜**: {ctx['original_data']['paraname']}\n"
            f"**å†…å®¹**: {ctx['answer'][:500]}..."
            for i, ctx in enumerate(best_context)
        ])
        
        prompt = f"""åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å’Œä¸­é—´å›ç­”ï¼Œæ’°å†™ä¸€ç¯‡ä¸“ä¸šã€ç»“æ„å®Œæ•´çš„æŠ€æœ¯æ–‡ç« ï¼š

{self._get_document_context()}

**æ–‡ç« ä¸»é¢˜**: {user_input}

**ä¸­é—´å›ç­”**:
{self.intermediate_response}

**å‚è€ƒä¿¡æ¯**:
{context_str}

**å†™ä½œè¦æ±‚**:
1. æ–‡ç« æ ‡é¢˜è¦æœ‰å¸å¼•åŠ›, åˆä¸å¤±ä¸“ä¸šæ€§
2. ç»“æ„åŒ…å«ï¼šéµå¾ªé‡‘å­—å¡”æ€ç»´æ¨¡å‹ï¼Œä»¥æ•…äº‹æˆ–è€…ç–‘é—®å…¥æ‰‹ï¼Œå¹¶å¯ä»¥å…ˆç»™å‡ºç­”æ¡ˆï¼Œå†æ ¸å¿ƒå†…å®¹ï¼ˆåˆ†å¤šä¸ªå°èŠ‚ï¼‰ã€æœ€åå‘¼åº”å¼€å¤´
3. å†…å®¹æ·±åº¦å’ŒæŠ€æœ¯å‡†ç¡®æ€§å¹¶é‡
4. å­—æ•°1500-2500å­—
5. ä½¿ç”¨Markdownæ ¼å¼ï¼ŒåŒ…å«é€‚å½“çš„æ ‡é¢˜å±‚çº§
6. ç¡®ä¿é€»è¾‘è¿è´¯ï¼Œä¿¡æ¯å‡†ç¡®
7. å¯ä»¥é€‚å½“æ‰©å±•å’Œè¡¥å……ç›¸å…³çŸ¥è¯†, åªèƒ½è¡¥å……æŠ€æœ¯äº‹å®æ€§çŸ¥è¯†ï¼Œå…¶ä»–ä¸è¦è¡¥å……ï¼
8. ä¸è¦ç›´æ¥å¤åˆ¶ä¸­é—´å›ç­”ï¼Œè€Œæ˜¯å°†å…¶ä½œä¸ºåŸºç¡€è¿›è¡Œæ¶¦è‰²
9. åŸºäºæŠ€æœ¯äº‹å®è¿›è¡Œé™ˆè¿°ï¼Œç»“åˆä½ å·²æœ‰çš„çŸ¥è¯†å’Œæ£€ç´¢åˆ°çš„ä¿¡æ¯
10. æ£€ç´¢åˆ°çš„ä¿¡æ¯æ˜¯æŠ€æœ¯è§„èŒƒæ–‡æ¡£ä¸­çš„æ®µè½ï¼Œéœ€è¦ç¡®ä¿å†…å®¹çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
11. ç»“åˆæ–‡æ¡£ç±»å‹ç‰¹ç‚¹è¿›è¡Œä¸“ä¸šåˆ†æå’Œé˜è¿°
12. åœ¨åˆé€‚å’Œå¯è¡Œçš„åœ°æ–¹å¯ä»¥è€ƒè™‘ç”¨mermaidå¯è§†åŒ–ï¼Œå®ç¼ºæ¯‹æ»¥ï¼Œä¸å¼ºæ±‚


è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„æ–‡ç« å†…å®¹ï¼š"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±æŠ€æœ¯ä½œå®¶ï¼Œæ“…é•¿æ’°å†™æ·±åº¦æŠ€æœ¯æ–‡ç« ï¼Œèƒ½å¤Ÿå°†å¤æ‚çš„æŠ€æœ¯æ¦‚å¿µè®²è§£å¾—æ¸…æ™°æ˜“æ‡‚ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        print("æ­£åœ¨è°ƒç”¨DeepSeek APIç”Ÿæˆé«˜è´¨é‡æ–‡ç« ...")
        article = self.deepseek_client.chat_completion(messages, temperature=0.8, max_tokens=4000)
        
        # ä»æ–‡ç« ä¸­æå–æ ‡é¢˜
        title = self._extract_title_from_article(article)
        print(f"æå–çš„æ–‡ç« æ ‡é¢˜: {title}")
        
        return article, title

# ä¸»ç¨‹åº
def main():
    # é…ç½®å‚æ•°
    JSON_FILES =  ["qa_data_mac2024.new.suffix.json", "11ax.suffix.json", "qa_data_11be.suffix.json"]#, "qa_data.mac-qa.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES =  ["qa_data.mac-qa.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES =  ["qa_data_mac2024.new.suffix.json"]#, "qa_data.mac-qa.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES = ["qa_data_mac2024.json"]
    #JSON_FILES = ["qa_data_802.11bn-pdt-mac-dbe-part-2.json","qa_data_802.11bn-pdt-mac-dbe.json"]
    #JSON_FILES = ["qa_data_802.11bn_pdt-mac-co-tdma-part-1.json","qa_data_802.11bn_pdt-mac-co-tdma-part-2.json","qa_data_802.11bn_pdt-mac-co-tdma-part-3.json"]
    #JSON_FILES = ["qa_data_802.11bn_pdt-mac-on-seamless-roaming-part-1.json", "qa_data_802.11bn_pdt-mac-on-seamless-roaming-part-2.json", "qa_data_802.11bn_pdt-mac-on-seamless-roaming-part-3.json", "qa_data_802.11bn_pdt-mac-on-seamless-roaming-part-4.json", "qa_data_802.11bn_pdt-mac-on-seamless-roaming-part-5.json"]  # æ›¿æ¢ä¸ºä½ çš„JSONæ–‡ä»¶åˆ—è¡¨
    #JSON_FILES =  ["CUDA_C_Programming_Guide_v12.1-19-498.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES =  ["BT_Core_specification_v5.3-181-3085.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES =  ["USB_3.0_R1.0-29-440.json", "USB4_1.0_with_errata_through_20201015-CLEAN-36-560.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES =  ["USB_3.0_R1.0-29-440.json", "USB4_1.0_with_errata_through_20201015-CLEAN-36-560.json", "PCI_Express_Base_Specification_Revision_6.0-99-1520.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES =  ["PCI_Express_Base_Specification_Revision_6.0-99-1520.json"] #["qa_data_mac2024.new.json"]
    #JSON_FILES =  ["PCI_Express_Base_Specification_Revision_6.0-99-1520.json", "PCI_Express_Base_Specification_Revision_5.0_Version_1.0-89-1210.json"] #["qa_data_mac2024.new.json"]
    DEEPSEEK_API_KEY = "sk-e86cebed80e445d8a3b1a6e715d6d1f2"  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    BASE_INDEX_DIR = "vector_indexes"  # å‘é‡ç´¢å¼•ä¿å­˜ç›®å½•
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import sentence_transformers
    except ImportError:
        print("è¯·å®‰è£…sentence-transformers: pip install sentence-transformers")
        return
    
    print("=" * 60)
    print("é«˜çº§æ–‡ç« ç”Ÿæˆç³»ç»Ÿå¯åŠ¨")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ–‡ç« ç”Ÿæˆå™¨
    generator = ArticleGenerator(
        JSON_FILES, 
        DEEPSEEK_API_KEY, 
        similarity_threshold=0.3,  # ç›¸ä¼¼åº¦é˜ˆå€¼
        max_iterations=100,        # æœ€å¤§è¿­ä»£æ¬¡æ•°
        max_context_length=100000, # ä¸Šä¸‹æ–‡é•¿åº¦ä¸Šé™
        base_index_dir=BASE_INDEX_DIR
    )
    
    # ç”¨æˆ·è¾“å…¥
    user_input = input("è¯·è¾“å…¥æ‚¨æƒ³è¦ç”Ÿæˆæ–‡ç« çš„ä¸»é¢˜æˆ–é—®é¢˜: ")
    
    if not user_input.strip():
        print("è¾“å…¥ä¸èƒ½ä¸ºç©ºï¼")
        return
    
    # ç”Ÿæˆæ–‡ç« 
    start_time = time.time()
    article, title, used_paranames = generator.generate_article(user_input)
    end_time = time.time()
    
    print(f"\næ–‡ç« ç”Ÿæˆå®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
    
    # æ˜¾ç¤ºä½¿ç”¨çš„paraname
    print("\n=== ä½¿ç”¨çš„æ®µè½æ ‡é¢˜ (paraname) ===")
    for i, paraname in enumerate(used_paranames, 1):
        print(f"{i}. {paraname}")
    print(f"\næ€»å…±ä½¿ç”¨äº† {len(used_paranames)} ä¸ªæ®µè½æ ‡é¢˜")
    
    # ä½¿ç”¨æ–‡ç« æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
    # æ¸…ç†æ ‡é¢˜ä¸­çš„éæ³•æ–‡ä»¶åå­—ç¬¦
    safe_title = re.sub(r'[<>:"/\\|?*]', '', title)
    safe_title = safe_title.replace(' ', '_').replace('/', '_').replace('\\', '_')
    safe_title = safe_title[:100]  # é™åˆ¶æ–‡ä»¶åé•¿åº¦
    
    output_path = f"{safe_title}.md"
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(article)
    
    print(f"\næ–‡ç« å·²ç”Ÿæˆå¹¶ä¿å­˜åˆ°: {output_path}")
    print("\n=== æ–‡ç« é¢„è§ˆ ===\n")
    print(article[:800] + "..." if len(article) > 800 else article)
    print(f"\næ–‡ç« æ€»é•¿åº¦: {len(article)} å­—ç¬¦")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†æ“ä½œï¼Œç¨‹åºå·²é€€å‡ºã€‚")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
